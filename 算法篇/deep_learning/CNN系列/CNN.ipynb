{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN简介  \n",
    "卷积神经网络（Convolutional Neural Network，CNN）是一种前馈神经网络，它的神经元可以响应一部分覆盖范围内的周围单元，对于图像有出色的表现。相比较于全连接网络，还包括卷积层（convolutional layer）和池化层（pooling layer）。\n",
    "\n",
    "### 1.1 卷积运算 \n",
    "以**Conv2D**为例   \n",
    "卷积神经网络接收形状为`(image_height, image_width, image_channels)`的输入张量（不包括批量维度）。对于包含两个空间轴（高度和宽度）和一个深度轴（通道轴）的3D张量，其卷积也称之为**特征图(feature map)**。输出的特征图也是仍然是一个3D张量，具有高度和宽度，其深度可以是任意取值（设定的参数），此时深度不再像RGB图像那样表示特定的颜色，而是代表**过滤器**。可以理解为过滤器对输入的数据的某一方面进行编码。   \n",
    "\n",
    "> Note: 特征图表示某个过滤器（卷积核）在输入中不同位置的响应，深度轴上每个维度都是一个特征\n",
    "\n",
    "- **卷积工作原理**    \n",
    "在3D输入特征图上滑动，按照卷积核的大小进行滑动，在每个可能的位置停止并提取周围特征的3D图块（window_height, window_width, input_depth）。然后每个3D图块与学到的同一个权重矩阵（卷积核）做张量积，转换为(output_depth, )的1D向量。然后对这些向量进行空间重组，使其转换为形状(height, width, output_depth)的3D输出特征图。过程参考下图， \n",
    "![](../../../pics/卷积工作原理.png)\n",
    "\n",
    "#### 1.1.1 卷积中关键参数   \n",
    "- 卷积核的大小    \n",
    "从输入中提取的图块的大小，通常是$3 \\times 3$或$5 \\times 5$\n",
    "\n",
    "- 过滤器的数量    \n",
    "输出特征图的深度\n",
    "\n",
    "- 填充(padding)   \n",
    "假设有$6 \\times 6$的特征图（36个方块），如果以$3 \\times 3$的卷积核进行滑动，最终输出的特征图的尺寸是$4 \\times 4$，也就是说比输入的尺寸小了一些。如果希望输入和输出的特征图空间维度相同，那么可以使用**填充**。   \n",
    "填充的具体做法是在输入特征图的每一边添加适当数目的行和列，使得每个输入方块都能作为卷积窗口的中心。   \n",
    "所以padding参数有两个取值，valid和same，valid表示不使用填充，same表示填充后输出和输入的维度相同。\n",
    "\n",
    "- 步幅(stride)   \n",
    "卷积时，两个连续窗口的距离是卷积的一个参数，步幅，默认值为1，可以使用步进卷积(strided convolution)，即步幅大于1的卷积。    \n",
    "步幅为2意味着特征图的宽度和高度都被做了2倍下采样。虽然步进卷积对于一些模型可能有用，但在实践中很少使用。  \n",
    "为了对特征图进行下采样，通常不使用步幅，而是使用池化(max-pooling)。\n",
    "\n",
    "卷积的可视化图，\n",
    "<table style=\"width:100%; table-layout:fixed; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/arbitrary_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/same_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/no_padding_strides.gif\"></td>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/padding_strides.gif\"></td>\n",
    "    <td><img width=\"200px\" src=\"../../../pics/padding_strides_odd.gif\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### 1.1.2 卷积的性质  \n",
    "- 模式的空间层次结构       \n",
    "卷积神经网络可以学到模式的**空间层次结构(spatial hierarchies of patterns)**。往往开始的卷积层学习较小的局部模式（比如眼睛，嘴巴等），接下来的卷积层将学习组成上一层的特征模式，依次类推。这使得卷积神经网络可以有效地学习越来越复杂、抽象的视觉概念。（视觉世界从根本上具有空间层次结构）   \n",
    "\n",
    "- 平移不变性    \n",
    "全连接层从特征空间中学到的是全局模式，而**卷积层学到的局部模式**。如果学习到这个局部模式后，它可以在任何地方识别这个模式(视觉世界从根本上具有平移不变性)。这可以使得卷积神经网络可以高效地利用数据，使用较少的训练样本可以学到具有泛化能力的数据表示。\n",
    "\n",
    "### 1.2 最大池化运算  \n",
    "最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。它的概念与卷积类似，不同的是最大池化使用硬编码的max张量运算对局部图块进行变换，而卷积则是使用学到的线性变换（卷积核）。    \n",
    "从参数上看，卷积和最大池化的不同是最大池化通常使用$2 \\times 2$的窗口和步幅2，其目的是将图像下采样2倍，卷积则是使用$3 \\times 3$的窗口和步幅1。   \n",
    "\n",
    "#### 1.2.1 为什么不删除最大池化层，保留较大的特征图？    \n",
    "1）不利于学习特征的空间层级结构，如果去掉池化层，这样卷积神经网络从输入中提取的原始图像比例随着层数加深在减少，不足以相应任务的学习。    \n",
    "2）学习参数较多，最后一层参数在全连接后会很多。   \n",
    "\n",
    "#### 1.2.2 使用最大池化的作用   \n",
    "1）减少需要处理的元素的个数，加速网络的训练，防止过拟合   \n",
    "2）通过让连续池化层，可以让观测窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层级结构  \n",
    "\n",
    "#### 1.2.3 相较于平均池化，为什么最大池化更好\n",
    "特征中往往编码了某种模式下或概念在特征图中的不同位置是否存在，而最大值相较于平均值能够给出更多的信息，而平均值方法可能导致错过或淡化特征是否存在的信息。   \n",
    "合理的下采样策略是首先生成密集的特征图（无步进卷积），然后观察特征在每个小块上的最大激活，而不是查看输入的稀疏窗口（步进卷积）或者对输入图块取平均，这两种方式肯嗯导致错过或者淡化重要特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN网络结构   \n",
    "以**ImageNet**数据集的算法演变来总结CNN算法。大致有LeNet(1998),AlexNet(2012),ZFNet(2013),GoogleNet/Inception(2014),VGGNet(2014),ResNet(2015)。\n",
    "\n",
    "### 2.1 LeNet(1998)\n",
    "LeNet是卷积神经网络的祖师爷LeCun在1998年提出，用于解决手写数字识别的视觉任务。自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。\n",
    "\n",
    "\n",
    "#### 2.1.1 LeNet网络架构\n",
    "经典的LeNet-5结构图如下，\n",
    "![](../../../pics/LeNet.png)\n",
    "\n",
    "剖析：  \n",
    "- 第一层，输入层（input），输入图像是单通道的$32 \\times 32$大小的图像，张量形状为(1, 32, 32)  \n",
    "\n",
    "- 第二层，卷积层（conv1），卷积核尺寸为$5 \\times 5$，步幅(stride)为1，卷积核的数量为6，经过卷积核后的图像尺寸大小为$(32-5+1) \\times (32-5+1) \\rightarrow 28 \\times 28$，该层输出的形状为(6, 28, 28)  \n",
    "> Note: 第二层中，使用6个大小为5*5的卷积核，这部分参数为6×(5×5+1)=156，+1表示一个核有bias，总共有156×28×28=122304个连接，但是只需要学习156个参数，主要是通过权值共享实现\n",
    "\n",
    "- 第三层，最大化池化层（pool1），池化核的大小为$2 \\times 2$，步幅为2，池化操作后，图像尺寸减半，变为$14 \\times 14$，该层输出后的形状为(6, 14, 14)，训练参数为6×(2×2+1)，连接数量为6×(2×2+1)×14×14   \n",
    "\n",
    "- 第四层，卷积层（conv2），类似第二层，卷积核尺寸大小为$5 \\times 5$，步幅为1,卷积核数量为16,卷积后的图像大小为（14-5+1），该层输出后的形状为(16, 10, 10)\n",
    "> 疑问：第四层的输入不再是(6, 14, 14)，而是第二层中所有feature map或者几个feature map的组合\n",
    "\n",
    "- 第五层，池化层（pool2），类似第三层，池化核尺寸大小为$2 \\times 2$，步幅为2,池化操作后，图像尺寸减半，变为$5 \\times 5$，该层的输出后的形状为(16, 5, 5)    \n",
    "\n",
    "- 第六层，卷积层（conv3），卷积核的大小为$5 \\times 5$，步幅为1,卷积核数量为120,则输出后的形状为(120, 1, 1)\n",
    "- 第七层，全连接层，84个神经元（对应7×12的比特图）\n",
    "- 第八层，输出层，神经元数量为10（目标共有10个类别），然后经过`softmax`，最后输出概率。\n",
    "\n",
    "#### 2.1.2 代码实现   \n",
    "\n",
    "```python\n",
    "def LeNet():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6,(5,5),strides=(1,1),input_shape=(32,32,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(16,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(120, (5,5), strides=(1,1), padding='valid',activation='sigmoid')\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation='sigmoid'))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    return model\n",
    "```\n",
    "> Note: 上述代码copy网上代码，未实例验证，该工作后期补充  \n",
    "\n",
    "#### 2.1.3 参考资源   \n",
    "- [LeNet-5详解](https://cuijiahua.com/blog/2018/01/dl_3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 AlexNet(2012)\n",
    "AlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势一举夺冠，从此深度学习和卷积神经网络名声鹊起，深度学习的研究如雨后春笋般出现，AlexNet的出现可谓是卷积神经网络的王者归来。\n",
    "\n",
    "#### 2.2.1 AlexNet网络架构  \n",
    "![](../../../pics/AlexNet.png)\n",
    "Alex网络结构整体如上图所示，前五层为卷积层，后面三层为全连接层，最终输出的1000类。 \n",
    "\n",
    "#### 2.2.2 比较LeNet\n",
    "相比较于LeNet，AlexNet的卷积神经网络总流程固定，只是网络结构还有一些训练技巧上做了很大改进，\n",
    "主要的改进点：   \n",
    "- **更深的网络**   \n",
    "AlexNet共有5层卷积层和3个全连接层，均比LeNet要多   \n",
    "- **数据增强**   \n",
    "对原始图像做了随机裁剪，增加模型的泛化能力，避免过拟合   \n",
    "- **使用Relu**   \n",
    "使用Relu代替Sigmoid函数，加快收敛速度   \n",
    "- **使用dropout**   \n",
    "在网络中加入了dropout策略，dropout具体参考**算法篇/deep_learning/深度学习基础**\n",
    "- **Local Responce Normalization**    \n",
    "局部响应归一层的基本思路是，假如这是网络的一块，比如是 13×13×256， LRN 要做的就是选取一个位置，比如说这样一个位置，从这个位置穿过整个通道，能得到 256 个数字，并进行归一化。进行局部响应归一化的动机是，对于这张 13×13 的图像中的每个位置来说，我们可能并不需要太多的高激活神经元。但是后来，很多研究者发现 LRN 起不到太大作用，因为并不重要，而且我们现在并不用 LRN 来训练网络。\n",
    "\n",
    "#### 2.2.3 代码实现   \n",
    "```python\n",
    "def AlexNet():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000,activation='softmax'))\n",
    "    return model\n",
    "```\n",
    "\n",
    "### 2.3 ZFNet(2013)\n",
    "ZFNet是2013ImageNet分类任务的冠军，其网络结构基本和AlexNet一样，不同的是参数做了调整，性能较AlexNet提升了不少。ZF-Net只是将AlexNet第一层卷积核由11变成7，步长由4变为2，第3，4，5卷积层转变为384，384，256。这一年的ImageNet还是比较平静的一届，其冠军ZF-Net的名堂也没其他届的经典网络架构响亮。\n",
    "\n",
    "#### 2.3.1 ZFNet网络架构\n",
    "参考AlexNet网络架构图\n",
    "\n",
    "#### 2.3.1 代码实现\n",
    "```python\n",
    "def ZF_Net():\n",
    "    model = Sequential()  \n",
    "    model.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(224,224,3),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "    model.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "    model.add(Flatten())  \n",
    "    model.add(Dense(4096,activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(4096,activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(1000,activation='softmax'))  \n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 VGGNet(2014)\n",
    "VGG-Nets是由牛津大学VGG（Visual Geometry Group）提出，是2014年ImageNet竞赛定位任务的第一名和分类任务的第二名的中的基础网络。VGG可以看成是加深版的AlexNet，均是conv layer+FC layer的组合，从图像中提取CNN特征，VGGNet是首选的网络结构，在许多迁移学习任务中也有很好的表现。\n",
    "\n",
    "#### 2.4.1 VGGNet网络架构   \n",
    "下图是VGGNet的网络结构的ABCDE过程，\n",
    "![](../../../pics/VGG.png)\n",
    "\n",
    "为了解决初始化（权重初始化）的问题，VGG采用一种pre-training的方式，也就是说训练时首先训练A网络，其参数较少网络可以快速收敛，然后使用此时网络的参数作为BCDE网络的初始化权重。  \n",
    "在D网络中有16个weight layer（conv和fc层数，不包括max-pool层），此时网络称之为VGG-16，E网络称之为VGG-19。   \n",
    "下图是VGG-16的网络架构图，\n",
    "![](../../../pics/VGG-16.png)\n",
    "\n",
    "#### 2.4.2 VGG创新点 \n",
    "- **网络权重的初始化**   \n",
    "如上面提到的，首先训练A网络，然后利用A网络的参数作为初始化的权重。   \n",
    "\n",
    "- **卷积层更小的filter尺寸和间隔**  \n",
    "VGGNet使用的卷积核尺寸为1×1和3×3两种，步幅均为1。\n",
    "  - 3×3的卷积核的优点   \n",
    "    多个3×3的卷基层比一个大尺寸filter卷基层有更多的非线性，使得判决函数更加具有判决性\n",
    "  - 1×1卷积核的优点   \n",
    "    作用是在不影响输入输出维数的情况下，对输入进行线性形变，然后通过Relu进行非线性处理，增加网络的非线性表达能     力。\n",
    "    \n",
    "- **三个连续的3×3卷积层**   \n",
    "两个连续的3×3卷积相当于原来5×5的感受野，三个3×3相当于一个7×7。使用三个的主要优势在于，   \n",
    "   - 使用三个卷积层将有三个relu，而不是一个，网络会有更多非线性能力，使决策函数更有判别性   \n",
    "   - 减少参数。比如输入输出都是C个通道，使用3个3×3的conv需要（3×3×3×C×C）=27×C×C，使用7×7的1个conv则需要7×7×C×C=49×C×C。这可以看做是对7×7施加一个正则化，使它分解成3个3×3的卷积。\n",
    "   \n",
    "#### 2.4.3 VGG作者的三个结论\n",
    "\n",
    "- LRN层作用不大    \n",
    "- 越深的网络效果越好   \n",
    "- 1×1的卷积也是有效的，但是没有3×3的卷积好，大一些的卷积核可以学习更大的空间特征。\n",
    "\n",
    "#### 2.4.4 代码实现  \n",
    "VGG-16的代码实现   \n",
    "```python\n",
    "def VGG_16():   \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000,activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 GoogleNet(2014)\n",
    "GoogLeNet在2014的ImageNet分类任务上击败了VGGNet夺得冠军，其实力肯定是非常深厚的，GoogLeNet跟AlexNet，VGGNet这种单纯依靠加深网络结构进而改进网络性能的思路不一样，它另辟幽径，在加深网络的同时（22层），也在网络结构上做了创新，引入Inception结构代替了单纯的卷积+激活的传统操作（这思路最早由Network in Network提出）。GoogLeNet进一步把对卷积神经网络的研究推上新的高度。\n",
    "\n",
    "#### 2.5.1 GoogleNet网络结构   \n",
    "\n",
    "下面是googleNet的网络架构图，\n",
    "![](../../../pics/googleNet.png)  \n",
    "\n",
    "#### 2.5.2 GoogleNet创新点  \n",
    "googleNet相较于ZFNet和AlexNet等传统卷积网络来说，做了非常大的改进，不再是单纯的网络加深，而是在网络结构上做了很大的创新。\n",
    "\n",
    "- Inception Module\n",
    "最大创新就是设计了Inception，Inception图如下，\n",
    "![](../../../pics/Inception.png)\n",
    "\n",
    "    - Inception初期版本   \n",
    "    上图中，左边是初期版本，这里使用三个不同的patch size，1X1 3X3 5X5，主要是为了避免patch alignment问题和减少计算量。比如对三个patch分别使用padding=0,1,2进行卷积就可以让卷积后的输出具有相同的尺寸，而patch size较小的时候对应的参数相应也小一些，比如1x1=1,3x3=9,5x5=25，再往后参数数量的增长速度就更快了。由于池化层在卷积神经网络中的作用往往都是起关键作用的，所以Inception model里也加入了一个池化层。但这种设计不是必须的，只是为了更方便。  \n",
    "    但是初级的版本有个很大的缺点就是参数量和计算量会很大，而且将三个卷积层和一个池化层的输出拼接后的feature map数量会变得很大，随着网络层数的增加，模型会变得很复杂，变得难以训练。\n",
    "    - Inception改进版本  \n",
    "    上图中，右边是改进版本，改进版本主要是在初期版本中加入了1×1的卷积层，依次来降低维度，减少计算量。   \n",
    "\n",
    "以GoogLeNet的3a模块为例，输入的feature map是28×28×192，3a模块中1×1卷积通道为64，3×3卷积通道为128,5×5卷积通道为32，如果是左图结构，那么卷积核参数为1×1×192×64+3×3×192×128+5×5×192×321×1×192×64+3×3×192×128+5×5×192×32，而右图对3×3和5×5卷积层前分别加入了通道数为96和16的1×1卷积层，这样卷积核参数就变成了1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32）1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32），参数大约减少到原来的三分之一。\n",
    "GoogLeNet利用1×1的卷积降维后，得到了更为紧凑的网络结构，虽然总共有22层，但是参数数量却只是8层的AlexNet的十二分之一（当然也有很大一部分原因是去掉了全连接层）。 \n",
    "\n",
    "> **Note:**\n",
    "正如前面在对比AlexNet和VGGNets的结构时提到的，对于卷积核大小的选择是需要经验和大量实验才可以确定的，到底是选3×3呢，还是5×5或者7×7？这个问题并没有一种明确的思路。Inception做法是跳出直线加深网络层数的思路，通过增加“宽度”的方式增加网络复杂度，避免陷入卷积核选择的陷阱，让程序自己学习如何选择卷积核。\n",
    "\n",
    "- 中间层辅助Loss单元   \n",
    "为了避免梯度消失，另外，可以提高较低阶段分类器的判别力，这是在提供正则化的同时克服梯度消失问题。googleNet在不同的深度地方设置了三个Loss单元，在参数更新时，中间层的两个Loss单元乘以0.3和最后的Loss单元相加作为最终的损失，以此来训练网络，预测时可以不管这两个Loss单元。\n",
    "\n",
    "- 全连接层变为average-pooling    \n",
    "将后面的全连接层全部替换为简单的全局平均pooling，在最后参数会变的更少，可以加快训练过程。\n",
    "\n",
    "\n",
    "#### 2.5.3 googleNet的其它版本   \n",
    "- Inception-v2\n",
    "Inception-v2是在第一代的GoogleNet基础上加入了批标准化（Batch Normalization）技术。其具体做法是，对mini-batch中所有的信号量进行统一的归一化，使得一个批次中所有的信号量符合均值为0，方差为1的高斯分布。\n",
    "\n",
    "- Inception-v3   \n",
    "Inception-v3在之前的版本上又有提高。其最核心的思想是将卷积核操作继续分解成更小的卷积核。首先，比如，借鉴VGGNets的思路，5*5的卷积可以由连续2层3*3卷积所替代，这样既减少了参数数量，也进一步加快了计算速度。这样的好处是，在经过这样的转换后，不但参数数量进一步减少，计算速度更快，而且网络的深度也加深了，增加了非线性表达能力。\n",
    "\n",
    "\n",
    "#### 2.5.3 代码实现   \n",
    "\n",
    "```python\n",
    "def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=3,name=bn_name)(x)\n",
    "    return x\n",
    "\n",
    "def Inception(x,nb_filter):\n",
    "    branch1x1 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branch3x3 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "    branch3x3 = Conv2d_BN(branch3x3,nb_filter,(3,3), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branch5x5 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "    branch5x5 = Conv2d_BN(branch5x5,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n",
    "    branchpool = Conv2d_BN(branchpool,nb_filter,(1,1),padding='same',strides=(1,1),name=None)\n",
    "\n",
    "    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n",
    "\n",
    "    return x\n",
    "\n",
    "def GoogLeNet():\n",
    "    inpt = Input(shape=(224,224,3))\n",
    "    #padding = 'same'，填充为(步长-1）/2,还可以用ZeroPadding2D((3,3))\n",
    "    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x = Inception(x,64)#256\n",
    "    x = Inception(x,120)#480\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x = Inception(x,128)#512\n",
    "    x = Inception(x,128)\n",
    "    x = Inception(x,128)\n",
    "    x = Inception(x,132)#528\n",
    "    x = Inception(x,208)#832\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x = Inception(x,208)\n",
    "    x = Inception(x,256)#1024\n",
    "    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1000,activation='relu')(x)\n",
    "    x = Dense(1000,activation='softmax')(x)\n",
    "    model = Model(inpt,x,name='inception')\n",
    "    return model\n",
    "```\n",
    "\n",
    "#### 2.5.4 参考资源  \n",
    "[GoogLeNet](https://blog.csdn.net/qq_27464321/article/details/81254920)   \n",
    "[GoogleNet的个人理解](https://blog.csdn.net/sunlianglong/article/details/79956734)   \n",
    "[GoogLeNet学习心得](https://www.cnblogs.com/Allen-rg/p/5833919.html)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refercences\n",
    "\n",
    "[1] [CNN Architectures](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "\n",
    "[2] [CNN网络架构演进：从LeNet到DenseNet](https://www.cnblogs.com/skyfsm/p/8451834.html)   \n",
    "[3] [深度学习--论文回顾：VGG-net](https://blog.csdn.net/u014513323/article/details/85053096)   \n",
    "[4] [CNN经典模型汇总](https://blog.csdn.net/qq_26591517/article/details/79805884)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
