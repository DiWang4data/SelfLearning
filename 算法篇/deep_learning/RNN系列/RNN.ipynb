{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-RNN简介\" data-toc-modified-id=\"1.-RNN简介-1\">1. RNN简介</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-RNN基本结构\" data-toc-modified-id=\"1.1-RNN基本结构-1.1\">1.1 RNN基本结构</a></span></li><li><span><a href=\"#1.2-RNN的设计模式\" data-toc-modified-id=\"1.2-RNN的设计模式-1.2\">1.2 RNN的设计模式</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-每个时间步有输出,隐藏单元有循环连接\" data-toc-modified-id=\"1.2.1-每个时间步有输出,隐藏单元有循环连接-1.2.1\">1.2.1 每个时间步有输出,隐藏单元有循环连接</a></span></li><li><span><a href=\"#1.2.2-每个时间步有输出,隐藏单云无循环连接\" data-toc-modified-id=\"1.2.2-每个时间步有输出,隐藏单云无循环连接-1.2.2\">1.2.2 每个时间步有输出,隐藏单云无循环连接</a></span></li><li><span><a href=\"#1.2.3-隐藏单元有循环连接,最后一个时间步有输出\" data-toc-modified-id=\"1.2.3-隐藏单元有循环连接,最后一个时间步有输出-1.2.3\">1.2.3 隐藏单元有循环连接,最后一个时间步有输出</a></span></li></ul></li><li><span><a href=\"#1.3-RNN-Application\" data-toc-modified-id=\"1.3-RNN-Application-1.3\">1.3 RNN Application</a></span></li><li><span><a href=\"#1.4-RNN-Extensions\" data-toc-modified-id=\"1.4-RNN-Extensions-1.4\">1.4 RNN Extensions</a></span></li><li><span><a href=\"#1.5-参考资源\" data-toc-modified-id=\"1.5-参考资源-1.5\">1.5 参考资源</a></span></li></ul></li><li><span><a href=\"#2.-RNN-BPTT\" data-toc-modified-id=\"2.-RNN-BPTT-2\">2. RNN-BPTT</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-BPTT推导\" data-toc-modified-id=\"2.1-BPTT推导-2.1\">2.1 BPTT推导</a></span></li><li><span><a href=\"#2.2-梯度消失/爆炸\" data-toc-modified-id=\"2.2-梯度消失/爆炸-2.2\">2.2 梯度消失/爆炸</a></span></li><li><span><a href=\"#2.3-参考资源\" data-toc-modified-id=\"2.3-参考资源-2.3\">2.3 参考资源</a></span></li></ul></li><li><span><a href=\"#3.-LSTM(1997)\" data-toc-modified-id=\"3.-LSTM(1997)-3\">3. LSTM(1997)</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-LSTM内部结构\" data-toc-modified-id=\"3.1-LSTM内部结构-3.1\">3.1 LSTM内部结构</a></span></li><li><span><a href=\"#3.2-LSTM如何实现长短期记忆(遗忘门输入门的作用)\" data-toc-modified-id=\"3.2-LSTM如何实现长短期记忆(遗忘门输入门的作用)-3.2\">3.2 LSTM如何实现长短期记忆(遗忘门输入门的作用)</a></span></li><li><span><a href=\"#3.3-LSTM中的激活函数是否可以随意更换\" data-toc-modified-id=\"3.3-LSTM中的激活函数是否可以随意更换-3.3\">3.3 LSTM中的激活函数是否可以随意更换</a></span></li><li><span><a href=\"#3.4-伪代码\" data-toc-modified-id=\"3.4-伪代码-3.4\">3.4 伪代码</a></span></li><li><span><a href=\"#3.5-参考资源\" data-toc-modified-id=\"3.5-参考资源-3.5\">3.5 参考资源</a></span></li></ul></li><li><span><a href=\"#4.-GRU(2014)\" data-toc-modified-id=\"4.-GRU(2014)-4\">4. GRU(2014)</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-GRU-内部结构\" data-toc-modified-id=\"4.1-GRU-内部结构-4.1\">4.1 GRU 内部结构</a></span></li><li><span><a href=\"#4.2-GRU伪代码\" data-toc-modified-id=\"4.2-GRU伪代码-4.2\">4.2 GRU伪代码</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN简介\n",
    "循环神经网络(RNN, recurrent neural network)，是一类具有内部**环(loop)**的神经网络。不同于密集连接神经网络和卷积神经网络，RNN可以存在”记忆“，它可以记录输入和输入间的状态；它不同于前馈神经网络（将序列数据整体转换为向量然后一次性处理），它处理序列数据的方式是遍历所有的序列元素，并保存一个**状态(state)**，包含与已查看内容相关的序列。\n",
    "\n",
    "### 1.1 RNN基本结构\n",
    "RNN本质是一个**递推函数**, $h^{(t)}=f(h^{(t-1)};\\theta)$,结合当前输入$x^{(t)}$即为$h^{(t)}=f(h^{(t-1)}, x^{(t)};\\theta)$.    \n",
    "以下为标准RNN计算图，\n",
    "![](../../../pics/RNN.jpg) \n",
    "RNN在某一个环的计算公式为 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "s^{(t)} &= \\boldsymbol{W} \\cdot [h^{(t-1)};x^{(t)}] + \\boldsymbol{b} \\\\\n",
    "h^{(t)} &= f(s^{(t)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$\\boldsymbol{W},\\boldsymbol{b}$为权重，$h^{(t)}$为t时刻的输出，函数$f$表示激活函数\n",
    "> Note:一般$h(0)$初始化为0向量，使用`tanh`作为激活函数\n",
    "\n",
    "### 1.2 RNN的设计模式\n",
    "RNN通常有三种设计模式，  \n",
    "- 每个时间步均有输出，且隐藏单元间有循环连接   \n",
    "- 每个时间步都有输出，但是隐藏单元之间没有循环连接，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接   \n",
    "- 隐藏单元之间有循环连接，但只有最后一个时间步有输出   \n",
    "\n",
    "#### 1.2.1 每个时间步有输出,隐藏单元有循环连接\n",
    "这种模式即通常所说的RNN结构，这种结构在每个时间步均会有输出，所以其经常应用于**Seq2Seq**任务中，比如序列标注、机器翻译等，如下图所示\n",
    "![](../../../pics/RNN-设计模式1.png)\n",
    "\n",
    "#### 1.2.2 每个时间步有输出,隐藏单云无循环连接\n",
    "这种模式的表达能力弱于第一种，这种结构在每个时间步均有输出，但其隐藏单元不再有连接。   \n",
    "正是因为这种设计，该模式的每个时间步可以与其他时间步单独训练，从而实现并行化，如下图所示\n",
    "![](../../../pics/RNN-设计模式2.png)\n",
    "\n",
    "#### 1.2.3 隐藏单元有循环连接,最后一个时间步有输出  \n",
    "不同于上面的两种模式（每个时间步均有输出），该模式只有最后一个时间步有输出，这种网络一般用于概括序列。具体来说，就是产生固定大小的表示，用于下一步处理，在一些**Seq2One**中简单任务中，这种网络用的比较多,因为这些任务只需要关注序列的全局特征。\n",
    "\n",
    "> Note: 前两种RNN被称为Elman Network和Jordan Network，通常说的RNN指前者 \n",
    "> - Elman RNN   \n",
    "> $$\n",
    " h^{(t)} = tanh(\\boldsymbol{W}_hx^{(t)}+\\boldsymbol{U_h}h^{(t-1)}+b_h \\\\\n",
    " y^{(t)} = softmax(\\boldsymbol{W}_yh^{(t)}+b_y)\n",
    "  $$\n",
    "> - Jordan RNN   \n",
    "> $$\n",
    " h^{(t)} = tanh(\\boldsymbol{W}_hx^{(t)}+\\boldsymbol{U_h}y^{(t-1)}+b_h \\\\\n",
    " y^{(t)} = softmax(\\boldsymbol{W}_yh^{(t)}+b_y)\n",
    "  $$\n",
    "\n",
    "### 1.3 RNN Application  \n",
    "- Language Modeling and Generating Text   \n",
    "- Machine Translation  \n",
    "Machine Translation is similar to language modeling in that our input is a sequence of words in our source language (e.g. German). We want to output a sequence of words in our target language (e.g. English). A key difference is that our output only starts after we have seen the complete input, because the first word of our translated sentences may require information captured from the complete input sequence.  \n",
    "![](../../../pics/rnn_translation.png)\n",
    "\n",
    "- Speech Recognition   \n",
    "- Generating Image Descriptions  \n",
    "\n",
    "### 1.4 RNN Extensions\n",
    "- Bidirectional RNNs   \n",
    "- Deep (Bidirectional) RNNs   \n",
    "- LSTM networks  \n",
    "\n",
    "### 1.5 参考资源   \n",
    "- [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs\n",
    "](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)   \n",
    "\n",
    "- [RNN基本结构](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/B-%E4%B8%93%E9%A2%98-RNN.md#rnn-%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN-BPTT\n",
    "\n",
    "BPTT(Backpropagation Through Time),基于时间反向梯度传播算法，RNN是一种基于时序数据的的神经网络模型，因此传统的BP算法不适用该模型的优化，因此BPTT算法应运而生。\n",
    "\n",
    "### 2.1 BPTT推导\n",
    "简记RNN结构如下，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s_t &= Ux_t+Ws_{t-1} \\\\ \n",
    "h_t &= \\rm{tanh}(s_t) \\\\ \n",
    "z_t &= Vh_t \\\\\n",
    "\\hat{y_t} &= \\rm{softmax}(z_t) \n",
    "\\end{aligned}\n",
    "$$\n",
    "以`cross entrophy loss`定义损失，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t(y_t, \\hat{y_t}) &= -y_t \\rm{log}\\hat{y_t} \\\\\n",
    "E(y, \\hat{y}) &= \\sum\\limits_{t} E_t(y_t, \\hat{y_t}) \\\\\n",
    "&= -\\sum\\limits_{t}y_t \\log \\hat{y_t} \n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$y_t$是$t$步的真实值，$\\hat{y_t}$是预测值。通常完整句子表示一个训练样本，所以整体损失就是每一步损失之和，也就是$E(y,\\hat{y})$\n",
    "\n",
    "BPTT算法的整体优化思路与BP算法类似，RNN中，我们目的求得$\\frac{\\partial E}{\\partial U}, \\frac{\\partial E}{\\partial W}, \\frac{\\partial E}{\\partial V}$，在上面提到整体损失是每个时间步损失的和，所以说$\\frac{\\partial E}{\\partial U}=\\sum_t\\frac{\\partial E_t}{\\partial U}, \\frac{\\partial E}{\\partial W}=\\sum_t\\frac{\\partial E_t}{\\partial W}, \\frac{\\partial E}{\\partial V}=\\sum_t\\frac{\\partial E_t}{\\partial V}$\n",
    "接下来只需要求每个时刻的偏导数即可。  \n",
    "\n",
    "- $\\frac{\\partial E_t}{\\partial V}$的计算   \n",
    "根据链式法则，有\n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial V} = \\frac{\\partial E_t}{\\partial \\hat{y_t}} \\cdot \\frac{\\partial \\hat{y_t}}{\\partial z_t} \\cdot \\frac{\\partial z_t}{\\partial V} \n",
    "$$\n",
    "- $\\frac{\\partial E_t}{\\partial W}$的计算  \n",
    "由于$W$是各个时刻共享的，所以$t$时刻之前每个时刻$U$的变化对$E_t$均有贡献。  \n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial W} = \\sum\\limits_{k=0}^t {\\frac{\\partial E_k}{\\partial s_k} \\cdot \\frac{\\partial s_k}{\\partial W}}\n",
    "$$\n",
    "- $\\frac{\\partial E_t}{\\partial U}$的计算  \n",
    "计算方式类似$\\frac{\\partial E_t}{\\partial W}$  \n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial U} = \\sum\\limits_{k=0}^t {\\frac{\\partial E_k}{\\partial s_k} \\cdot \\frac{\\partial s_k}{\\partial U}}\n",
    "$$\n",
    "\n",
    "### 2.2 梯度消失/爆炸\n",
    "\n",
    "上面$\\frac{\\partial E_t}{\\partial W}$计算中，按照链式法则展开后有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E_t}{\\partial W} &= \\sum\\limits_{k=0}^t {\\frac{\\partial E_k}{\\partial \\hat{y_t}} \\cdot \\frac{\\partial \\hat{y_t}}{\\partial z_t} \\cdot \\frac{\\partial z_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial s_t} \\cdot \\frac{\\partial s_t}{\\partial s_k} \\cdot \\frac{\\partial s_k}{\\partial W}}  \\\\\n",
    "&= \\sum\\limits_{k=0}^t {\\frac{\\partial E_k}{\\partial \\hat{y_t}} \\cdot \\frac{\\partial \\hat{y_t}}{\\partial z_t} \\cdot \\frac{\\partial z_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial s_t} \\cdot \\left(\\prod\\limits_{j=k+1}^t{\\frac{\\partial s_j}{\\partial s_{j-1}}} \\right) \\cdot \\frac{\\partial s_k}{\\partial W}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "上式中，注意到$\\frac{\\partial s_j}{\\partial s_{j_1}}$是对向量进行求偏导，所以结果是一个矩阵(Jacobian matrix)。因为tanh激活函数将值映射到(-1, 1)，导数范围(0, 1)，sigmoid激活函数将值映射到(0, 1)，导数范围(0, 0.25)，可以证明矩阵的二阶范数的上界是1. 一旦当矩阵中的值接近饱和，当矩阵相乘时，其值就会指数级别下降，造成梯度消失，换言之，这种现象导致RNN不能学习到长期的依赖关系。对于前馈神经网络来说当层数非常深时，也会面临同样的问题，梯度消失。  \n",
    "解决方式之一便是替换激活函数，比如换为Relu，但这样虽然可以避免梯度消失的问题，但是存在梯度爆炸问题（问题本质是各个单元的参数共享，还是存在矩阵连乘的问题），所以一个改进的方式是将参数$W$初始化为单位矩阵。  \n",
    "> 为什么CNN中使用Relu较少出现上面的问题\n",
    "> 主要原因是CNN中每层的参数$W$不同，且在初始化时，是独立同分布的，可以在一定程度上可以相互抵消，即使多层之后较小可能出现上面的问题\n",
    "\n",
    "### 2.3 参考资源 \n",
    "- [BPTT的详细推导](https://www.cnblogs.com/wacc/p/5341670.html)   \n",
    "- [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM(1997)\n",
    "LSTM(Long Short-Term Memory)，长短期记忆神经网络，是循环神经网络的一种。在上面标准RNN结构中，由于存在梯度消失的问题，所起其难以学习到长期的依赖，LSTM的设计的**门机制**可以很大程度上避免梯度的消失，而学习到长期的依赖关系。    \n",
    "\n",
    "LSTM网络的框架仍然标准的RNN框架（下图），而和标准RNN框架不同的是其计算隐含层的状态。标准的RNN计算计算隐藏层是\n",
    "$s_t = f(x_t, s_{t-1}) = \\rm{tanh}(Ux_t+Ws_{t-1})$，其中$U,W$是参数，$x_t$是第$t$步的输入，$s_{t-1}$是$t-1$步的隐藏层计算的状态，而LSTM只是改进了函数$f$，可以理解$s_t = LSTM(x_t, s_{t-1})$，接下俩看LSTM的具体计算模式。\n",
    "<div> <img src=\"../../../pics/gru-lstm.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "\n",
    "### 3.1 LSTM内部结构  \n",
    "- LSTM在标准RNN结构上加入了**门控机制**来限制信息的流动。 \n",
    "传统的LSTM（下）和标准RNN（上）内部比较  \n",
    "\n",
    "<div align=\"center\"><img src=\"../../../pics/LSTM3-SimpleRNN.png\" style=\"zoom:70%\" width=\"700px\" /></div>\n",
    "<div align=\"center\"><img src=\"../../../pics/LSTM3-chain.png\" style=\"zoom:70%\" width=\"700px\" /></div>\n",
    "\n",
    "- 总体来说，LSTM中加入了三个门：**遗忘门f**,**输入门i**,**输出门o**，以及一个**内部记忆状态C** \n",
    "    - 遗忘门f   \n",
    "    遗忘门控制前一步记忆状态中有多少可以被以往，相当于之前的记忆状态多大比例遗忘，如下所示  \n",
    "    <div> <img src=\"../../../pics/LSTM3-focus-f.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "    - 输入门i   \n",
    "    输入门控制当前的状态多大程度可以更新至记忆状态，相当于从目前的记忆中抽取一定的比例添加至记忆状态中，如下所示  \n",
    "    <div> <img src=\"../../../pics/LSTM3-focus-i.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "    - 记忆状态C   \n",
    "    记忆状态由遗忘门和输入门共同决定，相当于遗忘门以往掉一部分信息，然后输入门再添加一些信息，如下所示   \n",
    "    <div> <img src=\"../../../pics/LSTM3-focus-C.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "    - 输出门o   \n",
    "    控制当前的输出多大程度上取决于当前的记忆状态，相当于从目前记忆状态中提取多大比例输出\n",
    "    <div> <img src=\"../../../pics/LSTM3-focus-o.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "    \n",
    "### 3.2 LSTM如何实现长短期记忆(遗忘门输入门的作用)\n",
    "LSTM主要通过遗忘门和输入门来实现长短期的记忆   \n",
    "- 如果当前时间点的状态中没有重要信息，遗忘门 f 中各分量的值将接近 1（f -> 1）；输入门 i 中各分量的值将接近 0（i -> 0）；此时过去的记忆将会被保存，从而实现**长期记忆**；   \n",
    "\n",
    "- 如果当前时间点的状态中出现了重要信息，且之前的记忆不再重要，则 f -> 0，i -> 1；此时过去的记忆被遗忘，新的重要信息被保存，从而实现**短期记忆**；\n",
    "\n",
    "- 如果当前时间点的状态中出现了重要信息，但旧的记忆也很重要，则 f -> 1，i -> 1    \n",
    "\n",
    "### 3.3 LSTM中的激活函数是否可以随意更换\n",
    "- 在 LSTM 中，所有控制门都使用 sigmoid 作为激活函数（遗忘门、输入门、输出门）  \n",
    "- 在计算候选记忆或隐藏状态时，使用双曲正切函数 tanh 作为激活函数   \n",
    "\n",
    "- **sigmoid的饱和性**   \n",
    "    - 所谓饱和性，即输入超过一定范围后，输出几乎不再发生明显变化了\n",
    "    - sigmoid 的值域为 (0, 1)，符合门控的定义\n",
    "        - 当输入较大或较小时，其输出会接近 1 或 0，从而保证门的开或关；\n",
    "        - 如果使用非饱和的激活函数，将难以实现门控/开关的效果。\n",
    "    - sigmoid 是现代门控单元中的共同选择。\n",
    "\n",
    "- **tanh的作用**   \n",
    "    - 使用 tanh 作为计算状态时的激活函数，主要是因为其值域为 (-1, 1)  \n",
    "        - 一方面，这与多数场景下特征分布以 0 为中心相吻合；\n",
    "        - 另一方面，可以避免在前向传播的时候发生数值问题（主要是上溢）\n",
    "    -  此外，tanh 比 sigmoid 在 0 附近有更大的梯度，通常会使模型收敛更快。\n",
    "> 早期，使用 h(x) = 2*sigmoid(x) - 1 作为激活函数，该激活函数的值域也是 (-1, 1)\n",
    "\n",
    "### 3.4 伪代码 \n",
    "```python\n",
    "def LSTM_CELL(prev_ct, prev_ht, input):\n",
    "    '''\n",
    "    cell of lstm network\n",
    "    prev_ct: 前一时刻记忆状态 \n",
    "    prev_ht: 前一时刻的输出的状态\n",
    "    input: 当前时刻的输入\n",
    "    :return \n",
    "      ht: 当前时刻隐藏层的状态\n",
    "      Ct:当前时刻记忆状态\n",
    "    '''\n",
    "    ft = forget_layer(prev_ht, input)  # 遗忘门，遗忘比例  \n",
    "    \n",
    "    it = input_layer(prev_ht, input)  # 输入门，更新比例  \n",
    "    candidate = candiate_layer(prev_ht, input)  # 当前输入和之前隐藏层所有内容\n",
    "    \n",
    "    Ct = ft*prev_ct + it*candidate  # 更新当前时刻的记忆状态  \n",
    "    \n",
    "    ot = output_layer(prev_ht, input)  # 输出门，输出比例 \n",
    "    ht = ot*tanh(Ct)  # 当前时刻的内容\n",
    "    return ht, Ct\n",
    "```\n",
    "### 3.5 参考资源\n",
    "- [Understanding LSTM Networks\n",
    "](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)   \n",
    "- [LSTM内部结构以及问题](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/B-%E4%B8%93%E9%A2%98-RNN.md#lstm-%E7%9A%84%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. GRU(2014)\n",
    "GRU(Gated Recurrent Unit)，和LSTM类似，也是带有门机制的循环神经网络。GRU中设计两个门，分别是更新门和重置门，其参数量要少于LSTM。  \n",
    "\n",
    "### 4.1 GRU 内部结构\n",
    "- GRU内部结构  \n",
    "GRU中某个CELL的示意图如下，\n",
    "<div> <img src=\"../../../pics/LSTM3-var-GRU.png\" style=\"zoom:80%\" width=\"700px\" /></div>\n",
    "相比较于LSTM，GRU将遗忘门和输入门合并为**更新门(update)z**，使用**重置门(reset)**r代替输出门。\n",
    "\n",
    "- 重置门   \n",
    "更新门用于控制前一时刻隐含层输出状态的比例，如下， \n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_t &= \\sigma(W_r\\cdot [h_{t-1},x_t]) \\\\\n",
    "\\hat{h_t} &= \\rm{tanh} (W \\cdot [r_t*h_{t-1}, x_t])\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- 更新门   \n",
    "控制前一时刻状态信息融合到当前信息的比例，如下，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_t &= \\sigma(W_z\\cdot [h_{t-1},x_t]) \\\\\n",
    "h_t &= (1-z_t)*h_{t-1} + z_t*\\hat{h_t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 4.2 GRU伪代码  \n",
    "```python\n",
    "def GRU_CELL(prev_ht, input):\n",
    "    '''\n",
    "    cell of GRU.\n",
    "    input param:\n",
    "        prev_ht: 前一时刻的输出的状态 \n",
    "        input: 当前时刻输入\n",
    "    return: \n",
    "        ht: 当前层输出的状态\n",
    "    '''\n",
    "    rt = reset_layer(prev_ht, x_t)  # 重置门，确定更新比例\n",
    "    \n",
    "    hht = candidate_layer(rt*prev_ht, x_t)  # 当前层的计算内容\n",
    "    \n",
    "    zt = update_layer(prev_ht, x_t)  # 更新门，确定更新比例 \n",
    "    ht = zt*hht + (1-zt)*prev_ht  # 当前层输出的状态\n",
    "    return ht\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "471px",
    "left": "174px",
    "top": "206px",
    "width": "260.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
