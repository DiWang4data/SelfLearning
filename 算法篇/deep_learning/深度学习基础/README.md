本部分notebook目录如下：
- 1.Data Representation
    - 1.1 张量介绍
    - 1.2 张量计算
        - 1.2.1 逐元素运算
        - 1.2.2 广播
        - 1.2.3 张量点积
- 2.BP(Back Propagation)
    - 2.1 数据假设
    - 2.2 神经网络假设
    - 2.3 神经网络运行(前馈)
    - 2.4 神经网络更新参数（反向传播，反馈）
    - 2.5 具体权重更新推导
    - 2.6 BP算法流程
    - 2.7 BP算法额外说明
- 3.优化算法
    - 3.1 惯性保持
        - 3.1.1 带动量的SGD
        - 3.1.2 Nesterov 动量
        - 3.1.3 momentum和NAG的比较
    - 3.2 自适应学习率
        - 3.2.1 AdaGrad(2011)
        - 3.2.2 RMSProp(2012)
        - 3.2.3 AdaDelta(2012)
        - 3.2.4 Adam(2014)
    - 3.3 算法可视化
    - 3.4 优化算法的选择
    - 3.5 相关阅读
- 4.激活函数
    - 4.1 激活函数的作用
    - 4.2 神经网络的万能近似定理
    - 4.3 激活函数的形象解释
    - 4.4 常见的激活函数
        - 4.4.1 ReLU及其扩展
        - 4.4.2 Sigmoid函数
        - 4.4.3 tanh函数
        - 4.4.4 softplus函数
        - 4.4.5 ReLU和Sigmoid比较
- 5.正则化
    - 5.1 批标准化(Batch Normalization)
        - 5.1.1 算法原理
        - 5.1.2 单个样本计算(移动平均)
        - 5.1.3 为什么训练时不采用移动平均
        - 5.1.4 BN的作用
        - 5.1.5 BN相关阅读
    - 5.2 L1/L2正则
    - 5.3 Dropout
        - 5.3.1 训练时流程
        - 5.3.2 预测时流程
        - 5.3.3 dropout的作用
- 6.References