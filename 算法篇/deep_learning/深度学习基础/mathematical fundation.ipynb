{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Foundation\n",
    "\n",
    "## 1. Data Representation\n",
    "\n",
    "### 1.1 张量介绍\n",
    "\n",
    "一般来说，当前所有机器学习系统使用**张量**(tensor)作为数据结构，它是一个数据容器，包含的数据几乎总是数值类型。矩阵是二维张量，它是矩阵向任意维度的推广，张量的维度(dimension)叫做轴(axis)。\n",
    "\n",
    "- 标量(`0D`张量)   \n",
    "仅包含一个数字的张量叫做标量(scalar)，也称之为标量张量、零维张量\n",
    "\n",
    "- 向量(`1D`张量)\n",
    "数字组成的数组叫作向量(vector)或者一维张量，如下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T11:37:04.152585Z",
     "start_time": "2019-04-04T11:37:04.143332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D tensor: [1 3 5 7]\n",
      "1D axis:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 3, 5, 7])\n",
    "print(\"1D tensor:\", x)\n",
    "print(\"1D axis: \", x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矩阵(`2D`张量)\n",
    "向量组成的数组叫做矩阵(matrix)或者二维张量，如下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T11:41:16.667646Z",
     "start_time": "2019-04-04T11:41:16.659098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D tensor: \n",
      " [[2 7 4]\n",
      " [1 7 2]]\n",
      "2D axis:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randint(10, size=(2, 3))\n",
    "print(\"2D tensor: \\n\", x)\n",
    "print(\"2D axis: \", x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T11:48:39.756484Z",
     "start_time": "2019-04-04T11:48:39.746052Z"
    }
   },
   "source": [
    "- `3D`张量\n",
    "将多个矩阵组成一个数组便可以得到一个3D张量，如下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T11:43:13.677897Z",
     "start_time": "2019-04-04T11:43:13.663044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D tensor: \n",
      " [[[7 7 6 9 4]\n",
      "  [0 7 4 8 7]\n",
      "  [9 9 1 1 9]]\n",
      "\n",
      " [[4 2 1 5 7]\n",
      "  [0 0 8 3 4]\n",
      "  [1 2 5 9 4]]]\n",
      "3D axis:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randint(10, size=(2, 3, 5))\n",
    "print(\"3D tensor: \\n\", x)\n",
    "print(\"3D axis: \", x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以依次类推到更高维度的张量，现实中的数据张量，\n",
    "\n",
    "- 结构化数据   \n",
    "2D张量，形状(samples, features)\n",
    "\n",
    "- 序列数据   \n",
    "3D张量，形状(samples, timesteps, features)\n",
    "\n",
    "- 图像数据   \n",
    "4D张量，形状(samples, channels, height, width)\n",
    "\n",
    "- 视频数据   \n",
    "5D张量，形状(samples, frames, channels, height, width)\n",
    "\n",
    "张量的属性，\n",
    "\n",
    "- 轴的个数(阶)   \n",
    "nD张量有n个轴\n",
    "\n",
    "- 形状   \n",
    "nD张量的形状为长度为n向量，比如3D张量的形状可以为(1, 2, 4), (2, 5, 7)等等\n",
    "\n",
    "- 数据类型   \n",
    "绝大多数情况下张量的类型为数值类型，很少为字符串类型，因为张量存储在预先分配的连续内存中，但是字符串长度是可变的，无法用这种方式存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 张量计算\n",
    "\n",
    "#### 1.2.1 逐元素运算\n",
    "\n",
    "逐元素计算(element-wise)，独立的对张量中的每一个元素进行计算，完全可以实现并行\n",
    "\n",
    "#### 1.2.2 广播\n",
    "如果两个不同形状的张量进行相加，较小的张量会被广播(broadcast)，来匹配较大张量的形状，通常广播包含两个小步骤，\n",
    "1）向较小的张量添加轴（广播轴），使其ndim和大的张量相同\n",
    "2）将较小的张量沿着新的轴重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T12:24:10.319689Z",
     "start_time": "2019-04-04T12:24:10.295262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor operation broadcast:\n",
      "x1:\n",
      " [[[1 0 8 9 6 1]\n",
      "  [5 7 5 4 2 7]\n",
      "  [8 5 1 8 4 8]\n",
      "  [8 7 8 7 8 1]]\n",
      "\n",
      " [[2 3 7 9 3 0]\n",
      "  [2 9 9 5 5 9]\n",
      "  [0 9 0 1 7 5]\n",
      "  [0 5 5 6 5 0]]]\n",
      "x2:\n",
      " [[9 4 5 7 9 0]\n",
      " [0 5 7 0 8 8]\n",
      " [6 8 1 1 5 5]\n",
      " [9 9 6 1 7 1]]\n",
      "x1 + x2 = \n",
      "\n",
      "[[[10  4 13 16 15  1]\n",
      "  [ 5 12 12  4 10 15]\n",
      "  [14 13  2  9  9 13]\n",
      "  [17 16 14  8 15  2]]\n",
      "\n",
      " [[11  7 12 16 12  0]\n",
      "  [ 2 14 16  5 13 17]\n",
      "  [ 6 17  1  2 12 10]\n",
      "  [ 9 14 11  7 12  1]]]\n",
      "x1 shape (2, 4, 6) x1 ndim 3\n",
      "x2 shape (4, 6) x2 ndim 2\n",
      "x shape (2, 4, 6) x ndim 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "x1 = np.random.randint(10, size=(2, 4, 6))\n",
    "x2 = np.random.randint(10, size=(4, 6))\n",
    "x = x1+x2\n",
    "\n",
    "print(\"tensor operation broadcast:\")\n",
    "print(\"x1:\\n\", x1)\n",
    "print(\"x2:\\n\", x2)\n",
    "\n",
    "print(\"x1 + x2 = \\n\")\n",
    "print(x)\n",
    "\n",
    "print(\"x1 shape {} x1 ndim {}\".format(x1.shape, x1.ndim))\n",
    "print(\"x2 shape {} x2 ndim {}\".format(x2.shape, x2.ndim))\n",
    "print(\"x shape {} x ndim {}\".format(x.shape, x.ndim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 张量点积\n",
    "一般在Numpy、Keras、Theano和Tensorflow中，使用`*`实现逐元素乘积，在Numpy和Keras中，使用`dot`来实现点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T12:28:38.322002Z",
     "start_time": "2019-04-04T12:28:38.294003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor operation product:\n",
      "x1:\n",
      " [[[9 2 8 6 4 0]\n",
      "  [6 9 5 1 0 7]\n",
      "  [0 1 9 4 0 4]\n",
      "  [5 2 2 5 2 3]]\n",
      "\n",
      " [[5 5 7 2 2 0]\n",
      "  [8 6 4 1 5 1]\n",
      "  [9 8 9 4 7 5]\n",
      "  [3 7 7 3 9 9]]]\n",
      "x2:\n",
      " [[9 6]\n",
      " [3 6]\n",
      " [3 4]\n",
      " [3 5]\n",
      " [6 6]\n",
      " [5 0]]\n",
      "x1 + x2 = \n",
      "\n",
      "[[[153 152]\n",
      "  [134 115]\n",
      "  [ 62  62]\n",
      "  [ 99  87]]\n",
      "\n",
      " [[ 99 110]\n",
      "  [140 135]\n",
      "  [211 200]\n",
      "  [177 157]]]\n",
      "x1 shape (2, 4, 6) x1 ndim 3\n",
      "x2 shape (6, 2) x2 ndim 2\n",
      "x shape (2, 4, 2) x ndim 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "x1 = np.random.randint(10, size=(2, 4, 6))\n",
    "x2 = np.random.randint(10, size=(6, 2))\n",
    "\n",
    "x = np.dot(x1, x2)\n",
    "\n",
    "print(\"tensor operation product:\")\n",
    "print(\"x1:\\n\", x1)\n",
    "print(\"x2:\\n\", x2)\n",
    "\n",
    "print(\"x1 + x2 = \\n\")\n",
    "print(x)\n",
    "\n",
    "print(\"x1 shape {} x1 ndim {}\".format(x1.shape, x1.ndim))\n",
    "print(\"x2 shape {} x2 ndim {}\".format(x2.shape, x2.ndim))\n",
    "print(\"x shape {} x ndim {}\".format(x.shape, x.ndim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BP(Back Propagation)\n",
    "在神经网络训练时，往往基于反向传播算法，通过优化算法来优化网络权重。\n",
    "通常说”BP网络“时，一般是指用BP算法训练的多层前馈神经网络。\n",
    "\n",
    "### 2.1 数据假设   \n",
    "对于给定的数据集$D=\\{(\\vec{x_1}, \\vec{y_1}), \\ldots, (\\vec{x_m}, \\vec{y_m})\\}, \\vec{x_i} \\in R^d, \\vec{y_i} \\in R^l$，相当于数据集包含$m$个样本，每个样本有$d$个属性和对应$l$个标签。   \n",
    "\n",
    "### 2.2 神经网络假设   \n",
    "假设网络结构为三层，分别为输入层、隐含层和输出层，其分别对应$d$个输入神经元、$q$个隐含神经元和$l$个输出神经元。假设输入层至隐含层的连接权重为$v_{ih}$，隐含层至输出层的连接权重为$w_{hj}$，隐含层的神经元阈值为$\\gamma_{h}$，输出层的神经元阈值为$\\theta_{j}$，显然$i \\in {1, 2, \\cdots, d}, h \\in {1, 2, \\cdots, q}, j \\in {1, 2, \\cdots, l}$    \n",
    "> Note：此处日后有时间可以加一张图，来绘制一下网络结构\n",
    "\n",
    "\n",
    "### 2.3 神经网络运行(前馈)      \n",
    "对于某一个训练样本，$(\\vec{x_k}, \\vec{y_k})$，首先经过加权进入至隐含层，$\\alpha_{h}=\\sum_{i=1}^n v_{ih} \\cdot x_i$，隐含层具体取值为$b_h = f(\\alpha_h - \\gamma_h)$，然后数据由隐含层进入至输出层，$\\beta_j=\\sum_{h=1}^q w_{hj} \\cdot b_h$，在输出层的具体输出为$\\hat{y_j}=f(\\beta_j-\\theta_j)$，其中函数$f$为sigmoid函数\n",
    "\n",
    "### 2.4 神经网络更新参数（反向传播，反馈）   \n",
    "假设我们采用的损失函数为均方损失，那么对于训练样本$(\\vec{x_k}, \\vec{y_k})$，其对应的损失则为，   \n",
    "$$L_k=\\frac{1}{2}\\sum_{j=1}^l(y_j-\\hat{y_j})^2$$\n",
    "上述神经网络中共对应$(d+l+1)q+l$个参数，其中输入层值隐含层权重数量为$d \\cdot q$，隐含层阈值数量为$q$，隐含层至输出层权重的数量为$q \\cdot l$，输入层阈值数量为$l$.    \n",
    "BP算法是基于梯度下降策略来对权重进行更新调整，大致形式为，\n",
    "$v = v - \\eta \\frac{\\partial L}{\\partial v}$\n",
    "\n",
    "### 2.5 具体权重更新推导 \n",
    "- 参数$w_{hj}的更新$   \n",
    "基于梯度下降策略，其大致形式为$w_{hj}=w_{hj}-\\eta \\frac{\\partial L_k}{\\partial w_{hj}}$，依据链式法则，有如下， \n",
    "$$\\frac{\\partial L_k}{w_{hj}} = \\frac{\\partial L_k}{\\partial \\hat{y_{j}}} \\cdot \\frac{\\partial \\hat{y_{j}}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial w_{hj}}$$\n",
    "其中，\n",
    "$\\frac{\\partial L_k}{\\partial \\hat{y_j}} = \\hat{y_j}-y_j$, \n",
    "$\\frac{\\partial \\hat{y_j}}{\\partial \\beta_j} = \\hat{y_j}(1-\\hat{y_j})$, \n",
    "$\\frac{\\beta_j}{\\partial w_{hj}} = b_h$\n",
    "所以，   \n",
    "$$\\frac{\\partial L_k}{w_{hj}} = (\\hat{y_j}-y_j) \\cdot (\\hat{y_j}(1-\\hat{y_j})) \\cdot b_h$$\n",
    "\n",
    "- 参数$\\theta_j$的更新   \n",
    "形式为$\\theta_j = \\theta_j - \\eta \\frac{\\partial L_k}{\\partial \\theta_j}$，其中， \n",
    "$\\frac{\\partial L_k}{\\partial \\theta_j} = \\frac{\\partial L_k}{\\partial \\hat{y_{j}}} \\cdot \\frac{\\partial \\hat{y_j}}{\\partial \\theta_j}=(\\hat{y_j} - y_j) \\cdot (\\hat{y_j}(\\hat{y_j}-1))$\n",
    "\n",
    "- 参数$v_{ih}$的更新   \n",
    "形式为$v_{ih} = v_{ih} - \\eta \\frac{\\partial L_k}{\\partial v_{ih}}$，依据链式法则，有如下，\n",
    "$$\\frac{\\partial L_k}{\\partial v_{ih}} = \\frac{\\partial \\alpha_{h}}{\\partial v_{ih}} \\cdot \\frac{\\partial b_h}{\\partial \\alpha_h} \\cdot \\sum_{j=1}^{l}\\frac{\\partial \\beta_j}{\\partial b_h}\\frac{\\partial \\hat{y_j}}{\\partial \\beta_j} \\frac{\\partial L_k}{\\partial \\hat{y_j}}$$ \n",
    "其中，\n",
    "$\\frac{\\partial \\alpha_{h}}{\\partial v_{ih}} = x_i$, \n",
    "$\\frac{\\partial b_h}{\\partial \\alpha_h} = b_h(1-b_h)$, \n",
    "$\\frac{\\partial \\beta_j}{\\partial b_h} = w_{hj}$,\n",
    "$\\frac{\\partial \\hat{y_j}}{\\partial \\beta_j} = \\hat{y_j}(1-\\hat{y_j})$,\n",
    "$\\frac{\\partial L_k}{\\partial \\hat{y_j}} = \\hat{y_j}-y_j$\n",
    "所以，   \n",
    "$$\\frac{\\partial L_k}{\\partial v_{ih}} = x_i \\cdot b_h(1-b_h) \\cdot \\sum_{j=1}^{l}w_{hj}\\hat{y_j}(1-\\hat{y_j})(\\hat{y_j}-y_j)$$\n",
    "\n",
    "- 参数$\\gamma_h$的更新   \n",
    "形式为$\\gamma_h = \\gamma_h - \\eta \\frac{\\partial L_k}{\\partial \\gamma_h}$，依据链式法则， \n",
    "$$\\frac{\\partial L_k}{\\partial \\gamma_h} = \\frac{\\partial b_h}{\\partial \\gamma_h} \\cdot \\sum_{j=1}^{l}\\frac{\\partial \\beta_j}{\\partial b_h}\\frac{\\partial \\hat{y_j}}{\\partial \\beta_j} \\frac{\\partial L_k}{\\partial \\hat{y_j}}$$，\n",
    "根据上面**参数$v_{ih}$的更新**，\n",
    "$$\\frac{\\partial L_k}{\\partial \\gamma_h} = b_h(b_h-1) \\cdot \\sum_{j=1}^{l}w_{hj}\\hat{y_j}(1-\\hat{y_j})(\\hat{y_j}-y_j)$$\n",
    "\n",
    "### 2.6 BP算法流程\n",
    "BP算法具体框架如下，  \n",
    "\n",
    "------\n",
    "\n",
    "**输入：**   \n",
    "$\\ \\ \\ \\ $ 训练集$ D=\\{ (\\vec{x_k}, \\vec{y_k})\\}_{k=1}^m, \\vec{x_k} \\in R_d, \\vec{y_k} \\in R_l$   \n",
    "$\\ \\ \\ \\ $ 学习率$\\eta$    \n",
    "$\\ \\ \\ \\ $ 对应的损失函数$L$   \n",
    "**过程：**    \n",
    "1.在$(0, 1)$范围内随机初始化网络中所有的连接权值和阈值    \n",
    "2.**Repeat**  \n",
    "3.$\\ \\ \\ \\ $ **for all $(\\vec{x_k}, \\vec{y_k}) \\in D:$**   \n",
    "4.$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $根据**2.2.3 神经网络运行（前馈）**，求得对应输入$\\vec{x_k}$的输出$\\hat{y_k}$;   \n",
    "5.$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $分别求参数$w_{hj},\\theta_j,v_{ih}, \\gamma_h$对应的梯度值$\\frac{\\partial L_k}{\\partial w_{hj}}, \\frac{\\partial L_k}{\\partial \\theta_{j}}, \\frac{\\partial L_k}{\\partial v_{ih}}, \\frac{\\partial L_k}{\\partial \\gamma_h}$   \n",
    "6.$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $按照梯度下降的方式，更新参数$w_{hj},\\theta_j,v_{ih}, \\gamma_h$   \n",
    "7.$\\ \\ \\ \\ $ **end for**   \n",
    "8.**until**达到停止条件   \n",
    "**输出：**    \n",
    "连接权重和阈值，即$w_{hj},\\theta_j,v_{ih}, \\gamma_h$\n",
    "\n",
    "------\n",
    "\n",
    "### 2.7 BP算法额外说明\n",
    "上述**2.2.6 BP算法流程**中，每次针对一个训练样本更新权重和阈值，该过程也被称之为**标准BP算法**。需要注意的是，BP算法的目标是最小化训练集的损失，\n",
    "$$L = \\frac{1}{m}\\sum_{k=1}^{m}L_k$$\n",
    "如果上面基于此来推导连接权重和阈值，这样就称之为**累计误差逆传播算法**，两者之间的区别类似于**随机梯度下降(SGD)**和**标准梯度下降**。   \n",
    "为了达到相同的损失极小点，标准BP算法往往需要更多次数的迭代，累计BP算法需要读取整个训练集一遍后才对参数进行更新，其参数更新频率更新要低的多。  \n",
    "在大多数任务中，累计损失下降到一定程度后，进一步下降会非常缓慢，这时标准BP往往会获得较好的解。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 优化算法\n",
    "在目录`../../optimization_learning`中目前有一部分传统的优化算法，目前有**GradientDescent**，**CoordinateDescent**和**NewtonMethod**，本部分所介绍的优化算法，主要是深度学习中经常用到的，比如Adagrad，RMSProp，Adam等\n",
    "> TODO: 添加优化算法\n",
    "## 4. 激活函数\n",
    "## 5. 正则化\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
