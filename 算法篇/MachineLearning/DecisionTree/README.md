[TOC]

# 决策树

决策树是一种基本的分类与回归的方法，它可以认为是定义在特征空间与类空间上的**条件概率分布**，主要优点是模型具有可读性，分类速度快。

决策树的代表算法，主要是Quinlan在1986年提出ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。下面主要回顾这三种算法。

## 1. 决策树模型与学习

可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程：有决策树的根结点到叶结点的每一条路径构建一条规则；路径的内部结点的特征对应着规则的条件，而叶结点的类别对应着规则的结论。

决策树学习的策略是以损失函数为目标函数的最小化，当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题，因为从所有可能的决策树中选择最优决策树是**NP完全问题**，所以现实中决策树的学习算法通常采用启发式方法，近似求解这一最优问题，这样得到决策树是次最优（sub-optimal）的。

决策树的学习通常包含三个步骤，**特征选择**，**决策树生成**和**决策树的修剪**。

### 1.1 特征选择

特征选择，即从所有的特征中，选择一个“最优”的特征，放于决策树的结点中。这个“最优”的定义，主要是特征对于预测标签的预测能力或者区分能力。

假设某个特征为$A$,标签为$D$，那么这个预测能力可以用函数$f(A,D)$来表示，即当选择特征$A$时，特征$A$对标签的预测能力或者是区分能力。当标签$D$是类别变量时（分类任务），函数$f$往往是**信息增益**、**信息增益比**和**基尼增益**；而当标签$D$是连续变量时（回归任务），函数$f$往往采用**均方误差**。

### 1.2 决策树的生成

在第一步特征选择完成后，接下来需要确定该特征的划分，即找该特征的分割点，进行树的生长。

### 1.3 决策树的修剪

在以上两步完成后，这样的决策树往往对训练数据的分类很准确，但对未知数据集的预测能力有限，即出现过拟合现象。此时需要对构建的决策树进行剪枝（简化决策树）。

决策树的剪枝往往通过极小化决策树整体的损失函数或者代价函数来实现。通常损失函数往往定义为$C_{\alpha}(T)=C(T)+\alpha|T|$，其中$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂程度（叶子结点的个数），参数$\alpha \geq 0$控制两者之间的影响。较大的$\alpha$促使选择较简单的树，较小的$\alpha$促使选择复杂的树。$\alpha=0$表示不考虑模型的复杂度，只考虑模型的拟合程度。

剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$\alpha$确定时，子树越大，往往模型的拟合程度越高，但是模型的复杂程度越高；相反，子树越小，模型的复杂程度越低，但是此时模型的拟合程度不好，**损失函数恰好表示了两者的平衡**。

之所以决策树存在过拟合情况，就是在建树时，往往只考虑对训练数据更好的拟合，没有考虑减小模型的复杂程度，而决策树剪枝则通过优化损失函数，在保证一定数据拟合程度的基础上，降低模型复杂度，增加模型的泛化能力。可以说**决策树的生成是学习局部的模型，而决策树的剪枝是学习整体的模型**。

剪枝往往有两种策略，预剪枝和后剪枝。

- **预剪枝**

  在决策树的生成过程中，每增加一个结点，均考虑该结点增加前后的损失有无减小，如果损失增大，显然不适合继续生长。

- **后剪枝**

  在决策树的建树过程中不做干扰，在建树完成后，自底向上进行剪枝，如果去掉该结点后损失减小那么就将该结点去掉，反之则保留。

## 2. ID3算法

ID3算法主要解决的是分类任务，且特征限定为类别类型。

在特征选择中，使用**信息增益**来选择特征（使用信息增益来衡量特征对标签的预测能力），假设特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，

$$
f(A,D)=H(D)-H(D|A)
$$

> 其中$H(D)$表示变量$D$的信息熵，$H(D)=\sum_{i=1}^{d}p_i(1-p_i)$，$d$表示变量$D$中类别的个数，$H(D|A)$表示基于特征$A$的条件熵，$H(D|A)=\sum_{i=1}^{a}{\frac{|D_i|}{|D|}H(D_i)}$,$a$表示变量$A$中特征的个数。

在函数$f(A,D)$确定后，只需遍历所有特征，找到函数值（信息增益）最大的特征即可。

因为ID3算法中数据为类别类型的特征，所以在特征选择完成后，只需要按照该特征类别的个数进行分支，然后继续迭代该过程即可。显然ID3算法构建的决策树是一个多叉树，原生的ID3算法只有建树的过程，没有树的剪枝，所以该算法容易过拟合。

## 3. C4.5算法

C4.5算法主要解决的是分类任务，特征可以数值类型，也可以是连续类型，且特征中允许存在缺失值。

在特征选择中，适用**信息增益比**（信息增益倾向选择取值较多的特征）来选择特征，假定特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，

$$
f(A,D)=\frac{H(D)-H(D|A)}{H(D|A)}
$$

> 其中$H(D)$表示变量$D$的信息熵，$H(D)=\sum_{i=1}^{d}p_i(1-p_i)$，$d$表示变量$D$中类别的个数，$H(D|A)$表示基于特征$A$的条件熵，$H(D|A)=\sum_{i=1}^{a}{\frac{|D_i|}{|D|}H(D_i)}$,$a$表示变量$A$中特征的个数。

在函数$f(A,D)$确定后，只需遍历所有特征，找到函数值（信息增益比）最大的特征即可。

C4.5算法中，添加了对连续类型特征和缺失值的处理，

- 连续类型特征

  对于连续类型特征，采用**二分策略**，即遍历特征的取值，将特征划分为两个类别，然后求信息增益比，然后找到最优的切分点，以此作为该特征的切分点。

- 含缺失值的特征

  首先对该特征中未缺失的部分计算其信息增益比，然后将该特征的**未缺失率**乘以信息增益比，以此作为该特征最终的信息增益比。

> 需要注意的是，信息增益对取值较多的特征有所偏好，而信息增益比则对取值较少的特征有所偏好，所以后期，C4.5算法采用了一个启发式策略，先从候选特征中找出信息增益高于平均水平的特征，然后再选择信息增益比最高的。

## 4. CART算法

CART(classification and regression tree，分类与回归树)模型是由Breiman等人在1984年提出，是应用最广泛的决策树学习算法。从名字可以看出，CART算法可以处理分类和回归任务（ID3和C4.5算法只能处理分类任务）

## 5. 三种决策树算法的比较