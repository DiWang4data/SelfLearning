[TOC]

# CatBoost

CatBoost，类似XGBoost和LightGBM算法，是boosting算法框架一种，算法主要有两大特色，

- 处理类别类型特征
- 采用一种新的策略来生成leaf value，有助于降低过拟合

下面第一部分和第二部分分别介绍这两大特色。

## 1. Categorical features

### 1.1 类别变量转换为连续值

对类别变量的核心处理是将其类别替换为连续的值。

假设数据为$D=\{(\boldsymbol{X}_i,Y_i)\}_{i=1,\ldots,n},\ \boldsymbol{X}_i=(x_{i,1},\ldots, x_{i,m}) \in R^m, Y_i \in R$，即数据有$n$个样本，$m$个特征（包括连续变量和类别变量），$Y_i$是第$i$个样本的标签。

假设第$k$个特征为类别变量，一种简单的处理方式是将类别替换为对应标签的均值，表达如下
$$
x_{i,k}=\frac{\sum_{j=1}^n[x_{j,k}=x_{i,k}]\cdot Y_j}{\sum_{j=1}^n[x_{j,k}=x_{i,k}]}
$$
这种方式有一个较大的缺陷是容易发生标签泄露，比如某个类别只有一个样本，显然此时取值变为标签的取值。

为了避免上面的情况，catboost采用了一种方法是将所有样本随机打乱，然后依次**以累计的方式**用上面的公式计算，假设这个随机打乱为$\sigma = (\sigma_1,\ldots,\sigma_n)$，则
$$
x_{\sigma_p,k}=\frac{\sum_{j=1}^{p-1}[x_{\sigma_p,k}=x_{i,k}]\cdot Y_j + a \cdot P}{\sum_{j=1}^n[x_{\sigma_p,k}=x_{i,k}] + a}
$$
其中$P$和$a$是一个自定义的值，用来平滑低频次的类别。另外上面所提及的累计的方式主要体现在$x_{\sigma_p,k}$只与前$p-1$个样本有关。

上面处理是一种处理方式，一般来说替换规则是按照处理的任务进行区分，

- 回归任务

  通常时计算标签的均值来替换

- 分类任务

  通常用正样本的比例来替换

上面还存在的一个问题：上面是通过一个随机打乱来对类别变量进行处理，如果换一个打乱方式，显然计算取值会发生变化，一种简单的方式是做多个样本的打乱，但此时应该如何使用多个样本打乱，第二部分给出了一种方式。

### 1.2 特征组合

Catboost除了将类别特征转换为连续值外，还考虑了特征之间的组合（类别和类别变量间的组合和类别和连续值的组合），要穷举所有类别变量的组合显然不切实际，catboost采用了一种贪婪的方式来做，

在树的根节点创建时，未考虑特征组合，在之后的分割中考虑当前树中的数据集所有特征的特征组合，将这些特征组合值按照上面的方式转换为连续型的数值。

对于类别型变量和连续型变量也做了特征的组合：连续型特征会找到一个最优切割点，此时可以将连续变量看做一个具有两个类别的类别变量，然后再考虑和类别变量进行特征组合。

## 2. Fighting Gradient Bias

在类似gbdt算法框架中，每一步（每棵树）均来学习当前样本的梯度，这些梯度值在每一步均是由相同的样本通过当前的模型估计而得到，这种方式会导致梯度值分布发生偏移（有相关的论文做了分析，具体为什么发生偏移我不知道）。下面大致叙述一下如何改变这种情况，

大多数GBDT算法中，整体来看需要两个步骤，分别为**决策树的建立**和**设置叶子的得分**。

- 决策树的建立

  通常就是需要遍历所有的特征，以及特征所对应的的候选点集合，然后找到最优的特征分割点，递归进行增长

- 设置叶子的得分

  leaf value往往通过**梯度或者Newton step进行估计（？）**

在fighting gradient bias中，主要对第一种方式进行了改进。



## 3. 算法实现

