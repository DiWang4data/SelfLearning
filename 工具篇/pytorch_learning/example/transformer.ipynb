{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dropout=0.0):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
    "        \"\"\"前向传播.\n",
    "\n",
    "        Args:\n",
    "        \tq: Queries张量，形状为[B, L_q, D_q]\n",
    "        \tk: Keys张量，形状为[B, L_k, D_k]\n",
    "        \tv: Values张量，形状为[B, L_v, D_v]，一般来说就是k\n",
    "        \tscale: 缩放因子，一个浮点标量\n",
    "        \tattn_mask: Masking张量，形状为[B, L_q, L_k]\n",
    "\n",
    "        Returns:\n",
    "        \t上下文张量和attetention张量\n",
    "        \"\"\"\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))\n",
    "        if scale:\n",
    "        \tattention = attention * scale\n",
    "        if attn_mask:\n",
    "        \t# 给需要mask的地方设置一个负无穷\n",
    "        \tattention = attention.masked_fill_(attn_mask, -np.inf)\n",
    "\t\t# 计算softmax\n",
    "        attention = self.softmax(attention)\n",
    "\t\t# 添加dropout\n",
    "        attention = self.dropout(attention)\n",
    "\t\t# 和V做点积\n",
    "        context = torch.bmm(attention, v)\n",
    "        return context, attention\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=8, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "\n",
    "        self.dot_product_attention = ScaledDotProductAttention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\t\t# multi-head attention之后需要做layer norm\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, attn_mask=None):\n",
    "\t\t# 残差连接\n",
    "        residual = query\n",
    "\n",
    "        dim_per_head = self.dim_per_head\n",
    "        num_heads = self.num_heads\n",
    "        batch_size = key.size(0)\n",
    "\n",
    "        # linear projection\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "        query = self.linear_q(query)\n",
    "\n",
    "        # split by heads\n",
    "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "\n",
    "        if attn_mask:\n",
    "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "        # scaled dot product attention\n",
    "        scale = (key.size(-1) // num_heads) ** -0.5\n",
    "        context, attention = self.dot_product_attention(\n",
    "          query, key, value, scale, attn_mask)\n",
    "\n",
    "        # concat heads\n",
    "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
    "\n",
    "        # final linear projection\n",
    "        output = self.linear_final(context)\n",
    "\n",
    "        # dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        \"\"\"初始化。\n",
    "        \n",
    "        Args:\n",
    "            d_model: 一个标量。模型的维度，论文默认是512\n",
    "            max_seq_len: 一个标量。文本序列的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 根据论文给的公式，构造出PE矩阵\n",
    "        position_encoding = np.array([\n",
    "          [pos / np.pow(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)]\n",
    "          for pos in range(max_seq_len)])\n",
    "        # 偶数列使用sin，奇数列使用cos\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "\n",
    "        # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding\n",
    "        # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似\n",
    "        # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，\n",
    "        # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码\n",
    "        pad_row = torch.zeros([1, d_model])\n",
    "        position_encoding = torch.cat((pad_row, position_encoding))\n",
    "        \n",
    "        # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，\n",
    "        # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似\n",
    "        self.position_encoding = nn.Embedding(max_seq_len + 1, d_model)\n",
    "        self.position_encoding.weight = nn.Parameter(position_encoding,\n",
    "                                                     requires_grad=False)\n",
    "    def forward(self, input_len):\n",
    "        \"\"\"神经网络的前向传播。\n",
    "\n",
    "        Args:\n",
    "          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。\n",
    "\n",
    "        Returns:\n",
    "          返回这一批序列的位置编码，进行了对齐。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 找出这一批序列的最大长度\n",
    "        max_len = torch.max(input_len)\n",
    "        tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor\n",
    "        # 对每一个序列的位置进行对齐，在原序列位置的后面补上0\n",
    "        # 这里range从1开始也是因为要避开PAD(0)的位置\n",
    "        input_pos = tensor(\n",
    "          [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len])\n",
    "        return self.position_encoding(input_pos)\n",
    "    \n",
    "    \n",
    "class PositionalWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
    "        super(PositionalWiseFeedForward, self).__init__()\n",
    "        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "        self.w2 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w2(F.relu(self.w1(output)))\n",
    "        output = self.dropout(output.transpose(1, 2))\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(x + output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "\t\"\"\"Encoder的一层。\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=8, ffn_dim=2018, dropout=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self, inputs, attn_mask=None):\n",
    "\n",
    "        # self attention\n",
    "        context, attention = self.attention(inputs, inputs, inputs, padding_mask)\n",
    "\n",
    "        # feed forward network\n",
    "        output = self.feed_forward(context)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\t\"\"\"多层EncoderLayer组成Encoder。\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               vocab_size,\n",
    "               max_seq_len,\n",
    "               num_layers=6,\n",
    "               model_dim=512,\n",
    "               num_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "          [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in\n",
    "           range(num_layers)])\n",
    "\n",
    "        self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim, padding_idx=0)\n",
    "        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        output = self.seq_embedding(inputs)\n",
    "        output += self.pos_embedding(inputs_len)\n",
    "\n",
    "        self_attention_mask = padding_mask(inputs, inputs)\n",
    "\n",
    "        attentions = []\n",
    "        for encoder in self.encoder_layers:\n",
    "            output, attention = encoder(output, self_attention_mask)\n",
    "            attentions.append(attention)\n",
    "\n",
    "        return output, attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self, dataset, embedding):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.train_path = dataset + '/data/train.txt'                                # 训练集\n",
    "        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "        self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "        self.class_list = [x.strip() for x in open(\n",
    "            dataset + '/data/class.txt').readlines()]                                # 类别名单\n",
    "        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表\n",
    "        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果\n",
    "        self.log_path = dataset + '/log/' + self.model_name\n",
    "        self.embedding_pretrained = torch.tensor(\n",
    "            np.load(dataset + '/data/' + embedding)[\"embeddings\"].astype('float32'))\\\n",
    "            if embedding != 'random' else None                                       # 预训练词向量\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "        self.dropout = 0.5                                              # 随机失活\n",
    "        self.require_improvement = 2000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "        self.num_classes = len(self.class_list)                         # 类别数\n",
    "        self.n_vocab = 0                                                # 词表大小，在运行时赋值\n",
    "        self.num_epochs = 20                                            # epoch数\n",
    "        self.batch_size = 128                                           # mini-batch大小\n",
    "        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "        self.learning_rate = 5e-4                                       # 学习率\n",
    "        self.embed = self.embedding_pretrained.size(1)\\\n",
    "            if self.embedding_pretrained is not None else 300           # 字向量维度\n",
    "        self.dim_model = 300\n",
    "        self.hidden = 1024\n",
    "        self.last_hidden = 512\n",
    "        self.num_head = 5\n",
    "        self.num_encoder = 2\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n",
    "\n",
    "        self.postion_embedding = Positional_Encoding(config.embed, config.pad_size, config.dropout, config.device)\n",
    "        self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            copy.deepcopy(self.encoder)\n",
    "            # Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "            for _ in range(config.num_encoder)])\n",
    "\n",
    "        self.fc1 = nn.Linear(config.pad_size * config.dim_model, config.num_classes)\n",
    "        # self.fc2 = nn.Linear(config.last_hidden, config.num_classes)\n",
    "        # self.fc1 = nn.Linear(config.dim_model, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x[0])\n",
    "        out = self.postion_embedding(out)\n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = torch.mean(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, hidden, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)\n",
    "        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x)\n",
    "        out = self.feed_forward(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, pad_size, dropout, device):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    '''Scaled Dot-Product Attention '''\n",
    "    def __init__(self):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, scale=None):\n",
    "        '''\n",
    "        Args:\n",
    "            Q: [batch_size, len_Q, dim_Q]\n",
    "            K: [batch_size, len_K, dim_K]\n",
    "            V: [batch_size, len_V, dim_V]\n",
    "            scale: 缩放因子 论文为根号dim_K\n",
    "        Return:\n",
    "            self-attention后的张量，以及attention张量\n",
    "        '''\n",
    "        attention = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        # if mask:  # TODO change this\n",
    "        #     attention = attention.masked_fill_(mask == 0, -1e9)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, dropout=0.0):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        assert dim_model % num_head == 0\n",
    "        self.dim_head = dim_model // self.num_head\n",
    "        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.attention = Scaled_Dot_Product_Attention()\n",
    "        self.fc = nn.Linear(num_head * self.dim_head, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.fc_Q(x)\n",
    "        K = self.fc_K(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        # if mask:  # TODO\n",
    "        #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this\n",
    "        scale = K.size(-1) ** -0.5  # 缩放因子\n",
    "        context = self.attention(Q, K, V, scale)\n",
    "\n",
    "        context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
    "        out = self.fc(context)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Position_wise_Feed_Forward(nn.Module):\n",
    "    def __init__(self, dim_model, hidden, dropout=0.0):\n",
    "        super(Position_wise_Feed_Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
